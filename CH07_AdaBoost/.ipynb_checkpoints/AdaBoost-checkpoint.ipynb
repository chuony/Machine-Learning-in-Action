{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdaBoost算法：Boosting\n",
    "* 集成方法（元算法）：可以将不同的分类器组合起来\n",
    "* 集成形式：可以是不同算法的集成，也可以是同一算法在不同设置下的集成，还可以是数据集不同部分分配给不同分类器之后的集成。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 弱分类器：使用单层决策树（单节点的决策树）\n",
    "* 运行过程：训练数据中的每个样本，并赋予其一个权重，这些权重构成了向量D。一开始，这些权重都初始化成相等值。首先在训练数据上训练出一个弱分类器并计算该分类器的错误率，然后在同一数据集上再次训练弱分类器。在分类器的第二次训练当中，将会重新调整每个样本的权重，其中第一次分对的样本的权重将会降低，而第一次分错的样本的权重将会提高。为了从所有弱分类器中得到最终的分类结果，AdaBoost为每个分类器都分配了一个权重值alpha，这些alpha值是基于每个弱分类器的错误率进行计算的。\n",
    "* 错误率:$\\varepsilon=\\frac{未正确分类样本数}{所有样本数}$\n",
    "* alpha:$\\alpha=\\frac{1}{2}ln(\\frac{1-\\varepsilon}{\\varepsilon})$\n",
    "* 计算出alpha值之后，可以对权重向量D进行更新，以使得那些正确分类的样本的权重降低而错分样本的权重升高。\n",
    "* 在计算出D之后，AdaBoost又开始进入下一轮迭代。AdaBoost算法会不断地重复训练和调整权重的过程，直到训练错误率为0或者弱分类器的数目达到用户的指定值为止。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.基于单层决策树构建弱分类器\n",
    "* 单层决策树：仅基于单个特征来做决策(不管有多少特征，都只使用其中一个）。由于这棵树只有一次分裂过程，因此它实际上就是一个树桩。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建一个简单数据集\n",
    "from numpy import *\n",
    "def loadSimpData():\n",
    "    datMat = matrix([[ 1. ,  2.1],\n",
    "        [ 2. ,  1.1],\n",
    "        [ 1.3,  1. ],\n",
    "        [ 1. ,  1. ],\n",
    "        [ 2. ,  1. ]])\n",
    "    classLabels = [1.0, 1.0, -1.0, -1.0, 1.0]\n",
    "    return datMat,classLabels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "datMat,classLabels = loadSimpData()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "单层决策树生成函数\n",
    "* 第一个函数stumpClassify()是通过阈值比较对数据进行分类的。所有在阈值一边的数据会分到类别-1，而在另外一边的数据分到类别+1。\n",
    "* 该函数可以通过数组过滤来实现，首先将返回数组的全部元素设置为1，然后将所有不满足不等式要求的元素设置为-1。可以基于数据集中的任一元素进行比较，同时也可以将不等号在大于、小于之间切换。\n",
    "* 第二个函数buildStump()将会遍历stumpClassify()函数所有的可能输入值，并找到数据集上最佳的单层决策树。三层嵌套的for循环是程序最主要的部分。第一层for循环在数据集的所有特征上遍历。考虑到数值型的特征，我们就可以通过计算最小值和最大值来了解应该需要多大的步长。然后，第二层for循环再在这些值上遍历。甚至将阈值设置为整个取值范围之外也是可以的。因此，在取值范围之外还应该有两个额外的步骤。最后一个for循环则是在大于和小于之间切换不等式。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stumpClassify(dataMatrix,dimen,threshVal,threshIneq):\n",
    "    retArray = ones((shape(dataMatrix)[0],1))\n",
    "    if threshIneq == 'lt':\n",
    "        # 如果小于阈值，则赋值为-1\n",
    "        retArray[dataMatrix[:,dimen] <= threshVal] = -1.0\n",
    "    else:\n",
    "        # 如果大于阈值，则赋值为-1\n",
    "        retArray[dataMatrix[:,dimen] > threshVal] = -1.0\n",
    "    return retArray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildStump(dataArr,classLabels,D):\n",
    "    dataMatrix = mat(dataArr); labelMat = mat(classLabels).T\n",
    "    m,n = shape(dataMatrix)\n",
    "    numSteps = 10.0; bestStump = {}; bestClasEst = mat(zeros((m,1)))\n",
    "    # 最小误差初始化为正无穷大\n",
    "    minError = inf\n",
    "    # 遍历所有特征\n",
    "    for i in range(n):\n",
    "        # 找到特征中最大的值和最小的值\n",
    "        rangeMin = dataMatrix[:,i].min(); rangeMax = dataMatrix[:,i].max();\n",
    "        # 计算步长\n",
    "        stepSize = (rangeMax-rangeMin)/numSteps\n",
    "        for j in range(-1,int(numSteps)+1):\n",
    "            # lt:小于；gt:大于\n",
    "            for inequal in ['lt', 'gt']: #go over less than and greater than\n",
    "                threshVal = (rangeMin + float(j) * stepSize)\n",
    "                predictedVals = stumpClassify(dataMatrix,i,threshVal,inequal)\n",
    "                errArr = mat(ones((m,1)))\n",
    "                errArr[predictedVals == labelMat] = 0\n",
    "                weightedError = D.T*errArr  # 基于样本权重测错误率\n",
    "                #print \"split: dim %d, thresh %.2f, thresh ineqal: %s, the weighted error is %.3f\" % (i, threshVal, inequal, weightedError)\n",
    "                # 找到误差最小的分类方式\n",
    "                if weightedError < minError:\n",
    "                    minError = weightedError\n",
    "                    bestClasEst = predictedVals.copy() # 保存最好的分类信息\n",
    "                    # 保存最好的决策树信息\n",
    "                    bestStump['dim'] = i\n",
    "                    bestStump['thresh'] = threshVal\n",
    "                    bestStump['ineq'] = inequal\n",
    "    return bestStump,minError,bestClasEst\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'dim': 0, 'thresh': 1.3, 'ineq': 'lt'},\n",
       " matrix([[0.2]]),\n",
       " array([[-1.],\n",
       "        [ 1.],\n",
       "        [-1.],\n",
       "        [-1.],\n",
       "        [ 1.]]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D = mat(ones((5,1))/5)\n",
    "buildStump(datMat,classLabels,D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.完整Adaboost算法实现\n",
    "* 构建单层决策树时，错误率基于样本权重计算。则能保证每次最佳的单层决策树是不同的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adaBoostTrainDS(dataArr,classLabels,numIt=40):\n",
    "    # 收集不同的单层决策树桩的信息(不同的弱分类器)\n",
    "    weakClassArr = []\n",
    "    m = shape(dataArr)[0]\n",
    "    # 初始化权重\n",
    "    D = mat(ones((m,1))/m)   \n",
    "    aggClassEst = mat(zeros((m,1)))\n",
    "    for i in range(numIt):\n",
    "        # 构建单层决策树\n",
    "        bestStump,error,classEst = buildStump(dataArr,classLabels,D)\n",
    "        #print \"D:\",D.T\n",
    "        # 计算弱分类器的权重：max(error,eps)为了防止分母为0\n",
    "        alpha = float(0.5*log((1.0-error)/max(error,1e-16)))\n",
    "        bestStump['alpha'] = alpha\n",
    "        # 存储单层决策树到数组\n",
    "        weakClassArr.append(bestStump)      \n",
    "        #print \"classEst: \",classEst.T \n",
    "        # 计算e的指数项。如果分类错误则是alpha,分类正确是-alpha\n",
    "        expon = multiply(-1*alpha*mat(classLabels).T,classEst)\n",
    "        # 根据样本权重公式，更新样本权重\n",
    "        D = multiply(D,exp(expon))                              \n",
    "        D = D/D.sum()\n",
    "        # 更新累计类别估计值（包括每个训练好的分类器）\n",
    "        aggClassEst += alpha*classEst\n",
    "        #print \"aggClassEst: \",aggClassEst.T\n",
    "        # 计算误差：sign大于0则是1，小于0是-1。注意布尔值的逻辑运算（如果不等那误差就是1）\n",
    "        aggErrors = multiply(sign(aggClassEst) != mat(classLabels).T,ones((m,1)))\n",
    "        # 计算误差率\n",
    "        errorRate = aggErrors.sum()/m\n",
    "        print (\"total error: \",errorRate)\n",
    "        # 误差为0，则提前终止迭代（表示现在的效果已经很好，不用再迭代了）\n",
    "        if errorRate == 0.0: break\n",
    "    return weakClassArr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total error:  0.2\n",
      "total error:  0.2\n",
      "total error:  0.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'dim': 0, 'thresh': 1.3, 'ineq': 'lt', 'alpha': 0.6931471805599453},\n",
       " {'dim': 1, 'thresh': 1.0, 'ineq': 'lt', 'alpha': 0.9729550745276565},\n",
       " {'dim': 0, 'thresh': 0.9, 'ineq': 'lt', 'alpha': 0.8958797346140273}]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifierArr = adaBoostTrainDS(datMat,classLabels,9)  # 字典中是三个单层决策树\n",
    "classifierArr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.测试算法：基于AdaBoost的分类\n",
    "* 输入测试样本进行分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adaClassify(datToClass,classifierArr):\n",
    "    # datToClass:要分类的样本\n",
    "    dataMatrix = mat(datToClass) \n",
    "    m = shape(dataMatrix)[0]\n",
    "    aggClassEst = mat(zeros((m,1)))\n",
    "    for i in range(len(classifierArr)):\n",
    "        # 调用stumpClassify进行分类\n",
    "        classEst = stumpClassify(dataMatrix,classifierArr[i]['dim'],\\\n",
    "                                 classifierArr[i]['thresh'],\\\n",
    "                                 classifierArr[i]['ineq'])\n",
    "        aggClassEst += classifierArr[i]['alpha']*classEst\n",
    "        #print (aggClassEst)\n",
    "    return sign(aggClassEst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.69314718]]\n",
      "[[-1.66610226]]\n",
      "[[-2.56198199]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "matrix([[-1.]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adaClassify([0,0],classifierArr)  #随着迭代的进行，数据点[0,0]的分类结果越来越强"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 例：马疝病数据集上应用AdaBoost分类器\n",
    "https://zhuanlan.zhihu.com/p/327890396"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadDataSet(fileName):\n",
    "    numFeat = len(open(fileName).readline().split('\\t'))\n",
    "    dataMat = []; labelMat = []\n",
    "    fr = open(fileName)\n",
    "    for line in fr.readlines():\n",
    "        lineArr = []\n",
    "        curLine = line.strip().split('\\t')\n",
    "        for i in range(numFeat-1):\n",
    "            lineArr.append(float(curLine[i]))\n",
    "        dataMat.append(lineArr)\n",
    "        labelMat.append(float(curLine[-1]))\n",
    "    return dataMat,labelMat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataArr,labelArr = loadDataSet('horseColicTraining2.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total error:  0.2842809364548495\n",
      "total error:  0.2842809364548495\n",
      "total error:  0.24749163879598662\n",
      "total error:  0.24749163879598662\n",
      "total error:  0.25418060200668896\n",
      "total error:  0.2408026755852843\n",
      "total error:  0.2408026755852843\n",
      "total error:  0.22073578595317725\n",
      "total error:  0.24749163879598662\n",
      "total error:  0.23076923076923078\n"
     ]
    }
   ],
   "source": [
    "classifierArr = adaBoostTrainDS(dataArr,labelArr,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "testArr,testLabelArr = loadDataSet('horseColicTest2.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = adaClassify(testArr,classifierArr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16.0"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "errArr = mat(ones((67,1)))\n",
    "errArr[prediction!=mat(testLabelArr).T].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 不同的学习器数目，发现测试错误率在达到了一个最小值之后又开始上升了，模型过拟合。\n",
    "* 有文献声称，对于表现好的数据集，AdaBoost的测试错误率就会达到一个稳定值，并不会随着分类器的增多而上升。或许在本例子中的数据集也称不上“表现好”。\n",
    "* 该数据集一开始有30%的缺失值，对于Logistic回归而言，这些缺失值的假设就是有效的，而对于决策树却可能并不合适。如果回到数据集，将所有的0值替换成其他值，或者给定类别的平均值，那么能否得到更好的性能？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 总结\n",
    "* 很多人都认为，AdaBoost和SVM是监督机器学习中最强大的两种方法。实际上，这两者之间拥有不少相似之处。\n",
    "* 我们可以把弱分类器想象成SVM中的一个核函数，也可以按照最大化某个最小间隔的方式重写AdaBoost算法。而它们的不同就在于其所定义的间隔计算方式有所不同，因此导致的结果也不同。特别是在高维空间下，这两者之间的差异就会更加明显。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py37",
   "language": "python",
   "name": "py37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
