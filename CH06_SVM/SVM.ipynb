{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 支持向量机SVM\n",
    "* SVM需要掌握一些理论知识。通过将产品级代码和速度提升部分剥离出去，那么代码就变得可控，或许这样的代码就可以读懂了。\n",
    "* SVM是最好的现成的分类器，这里说的“现成”指的是分类器不加修改即可直接使用。同时，这就意味着在数据上应用基本形式的SVM分类器就可以得到低错误率的结果。SVM能够对训练集之外的数据点做出很好的分类决策。\n",
    "* SVM有很多实现，但是本章只关注其中最流行的一种实现，即序列最小优化（Sequential Minimal Optimization，SMO）算法，即一种求解支持向量机二次规划的算法。在此之后，将介绍如何使用一种称为核函数（kernel）的方式将SVM扩展到更多数据集上。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 超平面:分类的决策边界（N维）\n",
    "* 间隔 (margin):点到分隔面的距离被称为间隔\n",
    "* 支持向量:离分隔超平面最近的那些点\n",
    "* 基本原理：寻找最大间隔"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 基于优化问题的分类器求解过程：\n",
    "*　输入数据给分类器会输出一个类别标签，这相当于一个类似于Sigmoid的函数在作用。\n",
    "* 下面将使用类似海维赛德阶跃函数（即单位阶跃函数）的函数对wTx+b作用得到f(wTx+b)，其中当u<0,f(u)输出-1，反之则输出+1。这和前一章的Logistic回归有所不同，那里的类别标签是0或1。\n",
    "* 使用-1和1的理由而不用0和1：由于-1和+1仅仅相差一个符号，方便数学上的处理。我们可以通过一个统一公式来表示间隔或者数据点到分隔超平面的距离，同时不必担心数据到底是属于-1还是+1类。 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 间隔最大化（P111）:\n",
    "$\n",
    "argmax_{w,b}=\\{min(label*(w^Tx+b))*\\frac{1}{||w||}\\}\n",
    "$\n",
    "* 引入拉格朗日乘子法->引入松弛变量->SVM中的主要工作就是求解这些alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.SMO高效优化算法：SMO表示序列最小优化。\n",
    "* SMO算法是将大优化问题分解为多个小优化问题来求解的。这些小优化问题往往很容易求解，并且对它们进行顺序求解的结果与将它们作为整体来求解的结果是完全一致的。在结果完全相同的同时，SMO算法的求解时间短很多。\n",
    "* SMO算法的目标是求出一系列alpha和b，一旦求出了这些alpha，就很容易计算出权重向量w并得到分隔超平面。 \n",
    "* 工作原理是：每次循环中选择两个alpha进行优化处理。一旦找到一对合适的alpha，那么就增大其中一个同时减小另一个。这里所谓的“合适”就是指两个alpha必须要符合一定的条件，条件之一就是这两个alpha必须要在间隔边界之外，而其第二个条件则是这两个alpha还没有进行过区间化处理或者不在边界上。 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 应用简化版 SMO 算法处理小规模数据集 \n",
    "* 简化版：比完整的要慢；\n",
    "* 完整的SMO算法中的外循环确定要优化的最佳alpha对。而简化版却会跳过这一部分，首先在数据集上遍历每一个alpha，然后在剩下的alpha集合中随机选择另一个alpha，从而构建alpha对。\n",
    "* 这里比较重要的是，我们要同时改变两个alpha。之所以这样做是因为我们有一个约束条件。由于改变一个alpha可能会导致该约束条件失效，因此我们总是同时改变两个alpha。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 随机生成的数据 （testSet.txt 线性可分）,包含100个点。\n",
    "# 函数打开文件并对其进行逐行解析，从而得到每行的类标签和整个数据矩阵。\n",
    "def loadDataSet(fileName):\n",
    "    dataMat = []; labelMat = []\n",
    "    fr = open(fileName)\n",
    "    for line in fr.readlines():\n",
    "        lineArr = line.strip().split('\\t')\n",
    "        dataMat.append([float(lineArr[0]), float(lineArr[1])])\n",
    "        labelMat.append(float(lineArr[2]))\n",
    "    return dataMat,labelMat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 辅助函数：其中i是第一个alpha的下标，m是所有alpha的数目。\n",
    "# 只要函数值不等于输入值i，函数就会进行随机选择。 \n",
    "def selectJrand(i,m):\n",
    "    j=i \n",
    "    while (j==i):     # 要选择两个alpha进行优化，因此两个alpha不能一样。\n",
    "        j = int(random.uniform(0,m))\n",
    "    return j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 辅助函数：用于调整大于H或小于L的alpha值。\n",
    "def clipAlpha(aj,H,L):\n",
    "    if aj > H: \n",
    "        aj = H\n",
    "    if L > aj:\n",
    "        aj = L\n",
    "    return aj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataArr,labelArr = loadDataSet('testSet.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smoSimple(dataMatIn, classLabels, C, toler, maxIter):\n",
    "    # 输入参数：数据集、类别标签、常数C、容错率和最大的循环次数。\n",
    "    dataMatrix = mat(dataMatIn); labelMat = mat(classLabels).transpose()\n",
    "    b = 0; m,n = shape(dataMatrix)\n",
    "    alphas = mat(zeros((m,1)))    # 转换为列向量\n",
    "    iter = 0\n",
    "    while (iter < maxIter):\n",
    "        alphaPairsChanged = 0    # alphaPairsChanged用于记录alpha是否已经进行优化\n",
    "        for i in range(m):\n",
    "            fXi = float(multiply(alphas,labelMat).T*(dataMatrix*dataMatrix[i,:].T)) + b\n",
    "            Ei = fXi - float(labelMat[i]) # if语句中，要在误差范围内。同时检查alpha值，以保证其不能等于0或C。\n",
    "            if ((labelMat[i]*Ei < -toler) and (alphas[i] < C)) or ((labelMat[i]*Ei > toler) and (alphas[i] > 0)):\n",
    "                # 如果alpha可以更改则进入优化过程\n",
    "                j = selectJrand(i,m)      # 随机选择第二个alpha\n",
    "                fXj = float(multiply(alphas,labelMat).T*(dataMatrix*dataMatrix[j,:].T)) + b\n",
    "                Ej = fXj - float(labelMat[j])\n",
    "                alphaIold = alphas[i].copy(); alphaJold = alphas[j].copy(); # copy()方法：可以将新的alpha值与老的alpha值进行比较\n",
    "                # 保证alpha在0和C之间\n",
    "                if (labelMat[i] != labelMat[j]):\n",
    "                    L = max(0, alphas[j] - alphas[i])\n",
    "                    H = min(C, C + alphas[j] - alphas[i])\n",
    "                else:\n",
    "                    L = max(0, alphas[j] + alphas[i] - C)\n",
    "                    H = min(C, alphas[j] + alphas[i])\n",
    "                #if L==H: print (\"L==H\"); continue\n",
    "                # Eta是alpha[j]的最优修改量\n",
    "                eta = 2.0 * dataMatrix[i,:]*dataMatrix[j,:].T - dataMatrix[i,:]*dataMatrix[i,:].T - dataMatrix[j,:]*dataMatrix[j,:].T\n",
    "                #if eta >= 0: print (\"eta>=0\"); continue  # 进行了简化\n",
    "                alphas[j] -= labelMat[j]*(Ei - Ej)/eta\n",
    "                alphas[j] = clipAlpha(alphas[j],H,L)\n",
    "                #if (abs(alphas[j] - alphaJold) < 0.00001): print (\"j not moving enough\"); continue\n",
    "                # 对i进行修改，修改量与j相同，但方向相反\n",
    "                alphas[i] += labelMat[j]*labelMat[i]*(alphaJold - alphas[j]) \n",
    "                # 在对alpha[i]和alpha[j]进行优化之后，给这两个alpha值设置一个常数项b\n",
    "                b1 = b - Ei- labelMat[i]*(alphas[i]-alphaIold)*dataMatrix[i,:]*dataMatrix[i,:].T - labelMat[j]*(alphas[j]-alphaJold)*dataMatrix[i,:]*dataMatrix[j,:].T\n",
    "                b2 = b - Ej- labelMat[i]*(alphas[i]-alphaIold)*dataMatrix[i,:]*dataMatrix[j,:].T - labelMat[j]*(alphas[j]-alphaJold)*dataMatrix[j,:]*dataMatrix[j,:].T\n",
    "                if (0 < alphas[i]) and (C > alphas[i]): b = b1\n",
    "                elif (0 < alphas[j]) and (C > alphas[j]): b = b2\n",
    "                else: b = (b1 + b2)/2.0\n",
    "                alphaPairsChanged += 1\n",
    "                #print (\"iter: %d i:%d, pairs changed %d\" % (iter,i,alphaPairsChanged))\n",
    "        if (alphaPairsChanged == 0): iter += 1\n",
    "        else: iter = 0  # 检查alpha值是否做了更新，如果有更新则将iter设为0后继续运行程序。\n",
    "        #print (\"iteration number: %d\" % iter)\n",
    "        \n",
    "        # 只有在所有数据集上遍历maxIter次，且不再发生任何alpha修改之后，程序才会停止并退出while循环。\n",
    "    \n",
    "    return b,alphas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "b,alphas = smoSimple(dataArr,labelArr,0.6,0.001,40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0.127428  , 0.24143107, 0.36885907]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 命令是数组过滤的一个实例，而且它只对NumPy类型有用，却并不适用于Python中的正则表（regular list）。\n",
    "# 如果输入alpha>0，那么就会得到一个布尔数组，并且在不等式成立的情况下，其对应值为正确的。\n",
    "alphas[alphas>0]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[-3.83869975]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 3)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 支持向量的个数\n",
    "shape(alphas[alphas>0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4.658191, 3.507396] -1.0\n",
      "[3.457096, -0.082216] -1.0\n",
      "[6.080573, 0.418886] 1.0\n"
     ]
    }
   ],
   "source": [
    "# 为了解哪些数据点是支持向量，输入：\n",
    "for i in range(100):\n",
    "    if alphas[i]>0.0:print(dataArr[i],labelArr[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.利用完整 Platt SMO 算法加速优化\n",
    "* 不同：选择alpha的方式\n",
    "* 完整版的Platt SMO算法应用了一些能够提速的启发方法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.在复杂数据上应用核函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 径向基函数（radialbias function），高斯核函数\n",
    "def kernelTrans(X, A, kTup): \n",
    "    m,n = shape(X)\n",
    "    K = mat(zeros((m,1)))\n",
    "    if kTup[0]=='lin': K = X * A.T   # 线性核函数\n",
    "    elif kTup[0]=='rbf':\n",
    "        for j in range(m):\n",
    "            deltaRow = X[j,:] - A\n",
    "            K[j] = deltaRow*deltaRow.T\n",
    "        K = exp(K/(-1*kTup[1]**2)) \n",
    "    else: raise NameError('Houston We Have a Problem -- \\\n",
    "    That Kernel is not recognized')\n",
    "    return K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 建立一个数据结构来保存所有的重要值，而这个过程可以通过一个对象来完成。\n",
    "# 这里使用对象的目的并不是为了面向对象的编程，而只是作为一个数据结构来使用对象。\n",
    "# 在将值传给函数时，我们可以通过将所有数据移到一个结构中来实现，这样就可以省掉手工输入的麻烦了。\n",
    "# 而此时，数据就可以通过一个对象来进行传递。\n",
    "class optStruct:\n",
    "    def __init__(self,dataMatIn, classLabels, C, toler, kTup):  # 构建一个仅包含init方法的optStruct类。\n",
    "        self.X = dataMatIn\n",
    "        self.labelMat = classLabels\n",
    "        self.C = C\n",
    "        self.tol = toler\n",
    "        self.m = shape(dataMatIn)[0]\n",
    "        self.alphas = mat(zeros((self.m,1)))\n",
    "        self.b = 0\n",
    "        self.eCache = mat(zeros((self.m,2))) # 第一列给出的是eCache是否有效的标志位，而第二列给出的是实际的E值。\n",
    "        self.K = mat(zeros((self.m,self.m)))\n",
    "        for i in range(self.m):\n",
    "            self.K[:,i] = kernelTrans(self.X, self.X[i,:], kTup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 辅助函数：计算E值。单独写成函数因为后面经常用到，使用方便\n",
    "def calcEk(oS, k):\n",
    "    fXk = float(multiply(oS.alphas,oS.labelMat).T*oS.K[:,k] + oS.b)\n",
    "    Ek = fXk - float(oS.labelMat[k])\n",
    "    return Ek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 辅助函数：用于选择第二个alpha或者说内循环的alpha值\n",
    "# 目标：选择合适的第二个alpha值以保证在每次优化中采用最大步长。\n",
    "def selectJ(i, oS, Ei):         \n",
    "    maxK = -1; maxDeltaE = 0; Ej = 0\n",
    "    oS.eCache[i] = [1,Ei]  \n",
    "    validEcacheList = nonzero(oS.eCache[:,0].A)[0]\n",
    "    if (len(validEcacheList)) > 1:\n",
    "        for k in validEcacheList:   \n",
    "            if k == i: continue \n",
    "            Ek = calcEk(oS, k)\n",
    "            deltaE = abs(Ei - Ek)\n",
    "            if (deltaE > maxDeltaE):\n",
    "                maxK = k; maxDeltaE = deltaE; Ej = Ek   # 选择具有最大步长的j\n",
    "        return maxK, Ej\n",
    "    else:   \n",
    "        j = selectJrand(i, oS.m)\n",
    "        Ej = calcEk(oS, j)\n",
    "    return j, Ej"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 辅助函数：计算误差值并存入缓存当中。在对alpha值进行优化之后会用到这个值。 \n",
    "def updateEk(oS, k):\n",
    "    Ek = calcEk(oS, k)\n",
    "    oS.eCache[k] = [1,Ek]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 下面的代码几乎与smoSimple()函数一模一样，但是这里的代码已经使用了自己的数据结构。该结构在参数oS中传递。\n",
    "* 第二个重要的修改就是使用selectJ()而不是selectJrand()来选择第二个alpha的值。\n",
    "* 在alpha值改变时更新Ecache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 内循环：第二个alpha\n",
    "def innerL(i, oS):\n",
    "    Ei = calcEk(oS, i)\n",
    "    if ((oS.labelMat[i]*Ei < -oS.tol) and (oS.alphas[i] < oS.C)) or ((oS.labelMat[i]*Ei > oS.tol) and (oS.alphas[i] > 0)):\n",
    "        j,Ej = selectJ(i, oS, Ei)      \n",
    "        alphaIold = oS.alphas[i].copy(); alphaJold = oS.alphas[j].copy();\n",
    "        if (oS.labelMat[i] != oS.labelMat[j]):\n",
    "            L = max(0, oS.alphas[j] - oS.alphas[i])\n",
    "            H = min(oS.C, oS.C + oS.alphas[j] - oS.alphas[i])\n",
    "        else:\n",
    "            L = max(0, oS.alphas[j] + oS.alphas[i] - oS.C)\n",
    "            H = min(oS.C, oS.alphas[j] + oS.alphas[i])\n",
    "        #if L==H: print (\"L==H\"); return 0\n",
    "        eta = 2.0 * oS.K[i,j] - oS.K[i,i] - oS.K[j,j] \n",
    "        #if eta >= 0: print \"eta>=0\"; return 0\n",
    "        oS.alphas[j] -= oS.labelMat[j]*(Ei - Ej)/eta\n",
    "        oS.alphas[j] = clipAlpha(oS.alphas[j],H,L)\n",
    "        updateEk(oS, j) \n",
    "        #if (abs(oS.alphas[j] - alphaJold) < 0.00001): print \"j not moving enough\"; return 0\n",
    "        oS.alphas[i] += oS.labelMat[j]*oS.labelMat[i]*(alphaJold - oS.alphas[j])\n",
    "        updateEk(oS, i)                    \n",
    "        b1 = oS.b - Ei- oS.labelMat[i]*(oS.alphas[i]-alphaIold)*oS.K[i,i] - oS.labelMat[j]*(oS.alphas[j]-alphaJold)*oS.K[i,j]\n",
    "        b2 = oS.b - Ej- oS.labelMat[i]*(oS.alphas[i]-alphaIold)*oS.K[i,j]- oS.labelMat[j]*(oS.alphas[j]-alphaJold)*oS.K[j,j]\n",
    "        if (0 < oS.alphas[i]) and (oS.C > oS.alphas[i]): oS.b = b1\n",
    "        elif (0 < oS.alphas[j]) and (oS.C > oS.alphas[j]): oS.b = b2\n",
    "        else: oS.b = (b1 + b2)/2.0\n",
    "        return 1\n",
    "    else: return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 选择第一个alpha值的外循环\n",
    "def smoP(dataMatIn, classLabels, C, toler, maxIter,kTup=('lin', 0)):    # 完整的Platt SMO\n",
    "    oS = optStruct(mat(dataMatIn),mat(classLabels).transpose(),C,toler, kTup)\n",
    "    iter = 0\n",
    "    entireSet = True; alphaPairsChanged = 0\n",
    "    while (iter < maxIter) and ((alphaPairsChanged > 0) or (entireSet)):\n",
    "        alphaPairsChanged = 0\n",
    "        if entireSet:   # 遍历所有值\n",
    "            for i in range(oS.m):        \n",
    "                alphaPairsChanged += innerL(i,oS)\n",
    "                #print (\"fullSet, iter: %d i:%d, pairs changed %d\" % (iter,i,alphaPairsChanged))\n",
    "            iter += 1\n",
    "        else:          # 遍历非边界值\n",
    "            nonBoundIs = nonzero((oS.alphas.A > 0) * (oS.alphas.A < C))[0]\n",
    "            for i in nonBoundIs:\n",
    "                alphaPairsChanged += innerL(i,oS)\n",
    "                #print (\"non-bound, iter: %d i:%d, pairs changed %d\" % (iter,i,alphaPairsChanged))\n",
    "            iter += 1\n",
    "        if entireSet: entireSet = False \n",
    "        elif (alphaPairsChanged == 0): entireSet = True  \n",
    "        #print (\"iteration number: %d\" % iter)\n",
    "    return oS.b,oS.alphas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 得出的alpha值，计算w.基于alpha值得到超平面\n",
    "def calcWs(alphas,dataArr,classLabels):\n",
    "    X = mat(dataArr); labelMat = mat(classLabels).transpose()\n",
    "    m,n = shape(X)\n",
    "    w = zeros((n,1))\n",
    "    for i in range(m):\n",
    "        w += multiply(alphas[i]*labelMat[i],X[i,:].T)\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 利用核函数进行分类的径向基测试函数：径向基函数有一个用户定义的输入δ，需要确定他的大小然后利用该核函数构建出分类器。\n",
    "def testRbf(k1=1.3):\n",
    "    dataArr,labelArr = loadDataSet('testSetRBF.txt')\n",
    "    b,alphas = smoP(dataArr, labelArr, 200, 0.0001, 10000, ('rbf', k1)) # C=200\n",
    "    datMat=mat(dataArr); labelMat = mat(labelArr).transpose()\n",
    "    svInd=nonzero(alphas.A>0)[0]\n",
    "    sVs=datMat[svInd]                                              # 构建支持向量矩阵\n",
    "    labelSV = labelMat[svInd];\n",
    "    print (\"there are %d Support Vectors\" % shape(sVs)[0])\n",
    "    m,n = shape(datMat)\n",
    "    errorCount = 0\n",
    "    for i in range(m):\n",
    "        kernelEval = kernelTrans(sVs,datMat[i,:],('rbf', k1))\n",
    "        predict=kernelEval.T * multiply(labelSV,alphas[svInd]) + b\n",
    "        if sign(predict)!=sign(labelArr[i]): errorCount += 1\n",
    "    print (\"the training error rate is: %f\" % (float(errorCount)/m))\n",
    "    dataArr,labelArr = loadDataSet('testSetRBF2.txt')\n",
    "    errorCount = 0\n",
    "    datMat=mat(dataArr); labelMat = mat(labelArr).transpose()\n",
    "    m,n = shape(datMat)\n",
    "    for i in range(m):\n",
    "        kernelEval = kernelTrans(sVs,datMat[i,:],('rbf', k1))\n",
    "        predict=kernelEval.T * multiply(labelSV,alphas[svInd]) + b\n",
    "        if sign(predict)!=sign(labelArr[i]): errorCount += 1    \n",
    "    print (\"the test error rate is: %f\" % (float(errorCount)/m))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 18 Support Vectors\n",
      "the training error rate is: 0.180000\n",
      "the test error rate is: 0.250000\n"
     ]
    }
   ],
   "source": [
    "testRbf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 测试错误率、训练错误率、支持向量个数随k1的变化而变化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 例：手写识别问题回顾"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def img2vector(filename):\n",
    "    returnVect = zeros((1,1024))\n",
    "    fr = open(filename)\n",
    "    for i in range(32):\n",
    "        lineStr = fr.readline()\n",
    "        for j in range(32):\n",
    "            returnVect[0,32*i+j] = int(lineStr[j])\n",
    "    return returnVect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadImages(dirName):\n",
    "    from os import listdir\n",
    "    hwLabels = []\n",
    "    trainingFileList = listdir(dirName)          \n",
    "    m = len(trainingFileList)\n",
    "    trainingMat = zeros((m,1024))\n",
    "    for i in range(m):\n",
    "        fileNameStr = trainingFileList[i]\n",
    "        fileStr = fileNameStr.split('.')[0]     \n",
    "        classNumStr = int(fileStr.split('_')[0])\n",
    "        if classNumStr == 9: hwLabels.append(-1)\n",
    "        else: hwLabels.append(1)\n",
    "        trainingMat[i,:] = img2vector('%s/%s' % (dirName, fileNameStr))\n",
    "    return trainingMat, hwLabels    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据集中只包含1和9，二分类问题。并且标签为1和-1\n",
    "def testDigits(kTup=('rbf', 10)):\n",
    "    dataArr,labelArr = loadImages('trainingDigits')\n",
    "    b,alphas = smoP(dataArr, labelArr, 200, 0.0001, 10000, kTup)  #元组kTup是输入参数\n",
    "    datMat=mat(dataArr); labelMat = mat(labelArr).transpose()\n",
    "    svInd=nonzero(alphas.A>0)[0]\n",
    "    sVs=datMat[svInd] \n",
    "    labelSV = labelMat[svInd];\n",
    "    print (\"there are %d Support Vectors\" % shape(sVs)[0])\n",
    "    m,n = shape(datMat)\n",
    "    errorCount = 0\n",
    "    for i in range(m):\n",
    "        kernelEval = kernelTrans(sVs,datMat[i,:],kTup)\n",
    "        predict=kernelEval.T * multiply(labelSV,alphas[svInd]) + b\n",
    "        if sign(predict)!=sign(labelArr[i]): errorCount += 1\n",
    "    print (\"the training error rate is: %f\" % (float(errorCount)/m))\n",
    "    dataArr,labelArr = loadImages('testDigits')\n",
    "    errorCount = 0\n",
    "    datMat=mat(dataArr); labelMat = mat(labelArr).transpose()\n",
    "    m,n = shape(datMat)\n",
    "    for i in range(m):\n",
    "        kernelEval = kernelTrans(sVs,datMat[i,:],kTup)\n",
    "        predict=kernelEval.T * multiply(labelSV,alphas[svInd]) + b\n",
    "        if sign(predict)!=sign(labelArr[i]): errorCount += 1    \n",
    "    print (\"the test error rate is: %f\" % (float(errorCount)/m)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 40 Support Vectors\n",
      "the training error rate is: 0.000000\n",
      "the test error rate is: 0.016129\n"
     ]
    }
   ],
   "source": [
    "testDigits(('rbf',20))    #训练时间长"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 总结：\n",
    "* 支持向量机的泛化错误率较低，也就是说它具有良好的学习能力，且学到的结果具有很好的推广性。\n",
    "* 支持向量机是一个二类分类器。当用其解决多类问题时，则需要额外的方法对其进行扩展。\n",
    "* SVM的效果也对优化参数和所用核函数中的参数敏感。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py37",
   "language": "python",
   "name": "py37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
