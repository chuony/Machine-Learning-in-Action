{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 树回归（CART）\n",
    "* CART（Classification And Regression Trees，分类回归树）：既可以用于分类还可以用于回归\n",
    "* ID3算法：1.按照特征的所有可能取值来切分（如果特征有4个取值，那么数据被分成4份）。一旦按某特征切分后，该特征在之后的算法执行过程中将不会再起作用，所以有观点认为这种切分方式过于迅速。2.ID3不能处理连续性特征。只有事先将连续型特征转换成离散型，才能在ID3算法中使用。但这种转换过程会破坏连续型变量的内在性质。\n",
    "* CART：使用二元切分来处理连续型变量。二元切分法易于对树构建过程进行调整以处理连续型特征。具体的处理方法是：如果特征值大于给定值就走左子树，否则就走右子树。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.连续和离散型特征的树的构建\n",
    "* 在树的构建过程中，需要解决多种类型数据的存储问题。\n",
    "* 第3章用一部字典来存储每个切分，但该字典可以包含两个或两个以上的值。而CART算法只做二元切分，所以这里可以固定树的数据结构。\n",
    "* 可以用面向对象的编程模式来建立这个数据结构。\n",
    "* Python具有足够的灵活性，可以直接使用字典来存储树结构而无须另外自定义一个类，从而有效地减少代码量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class treeNode():\n",
    "    def __init__(self, feat, val, right, left):\n",
    "        featureToSplitOn = feat    # 待切分的特征\n",
    "        valueOfSplit = val         # 待切分的特征值\n",
    "        rightBranch = right        # 右子树。当不再需要切分的时候，也可以是单个值\n",
    "        leftBranch = left          # 左子树。与右子树类似"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 构建两种树：回归树（regression tree），其每个叶节点包含单个值；模型树（model tree），其每个叶节点包含一个线性方程。\n",
    "* 创建这两种树时尽量使得代码之间可以重用。下面先给出两种树构建算法中的一些共用代码。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CART算法的实现代码 \n",
    "from numpy import *\n",
    "\n",
    "def loadDataSet(fileName):      \n",
    "    dataMat = []                \n",
    "    fr = open(fileName)\n",
    "    for line in fr.readlines():\n",
    "        curLine = line.strip().split('\\t')\n",
    "        fltLine = list(map(float,curLine))        # 将每行映射为浮点数.python3加list\n",
    "        dataMat.append(fltLine)\n",
    "    return dataMat\n",
    "\n",
    "# 在给定特征和特征值的情况下，该函数通过数组过滤方式将上述数据集合切分得到两个子集并返回。 \n",
    "def binSplitDataSet(dataSet, feature, value):\n",
    "    # nonzero()返回的是为True的元素下标\n",
    "    mat0 = dataSet[nonzero(dataSet[:,feature] > value)[0],:] # 那一列符合要求的一整行\n",
    "    mat1 = dataSet[nonzero(dataSet[:,feature] <= value)[0],:]\n",
    "    return mat0,mat1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 决定树的类型：leafType给出建立叶节点的函数；errType代表误差计算函数；而ops是一个包含树构建所需其他参数的元组。 \n",
    "def createTree(dataSet, leafType=regLeaf, errType=regErr, ops=(1,4)):\n",
    "    # 满足停止条件时chooseBestSplit()返回叶节点值\n",
    "    # 如果构建的是回归树，该模型是一个常数。如果是模型树，其模型是一个线性方程。后面会看到停止条件的作用方式。\n",
    "    # 如果不满足停止条件，chooseBestSplit()将创建一个新的Python字典并将数据集分成两份，在这两份数据集上将分别继续递归调用createTree()函数。 \n",
    "    feat, val = chooseBestSplit(dataSet, leafType, errType, ops)\n",
    "    if feat == None: return val \n",
    "    retTree = {}                    # 还是用的字典？没用前面定义的类？\n",
    "    retTree['spInd'] = feat\n",
    "    retTree['spVal'] = val\n",
    "    lSet, rSet = binSplitDataSet(dataSet, feat, val)\n",
    "    retTree['left'] = createTree(lSet, leafType, errType, ops)\n",
    "    retTree['right'] = createTree(rSet, leafType, errType, ops)\n",
    "    return retTree  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[1., 0., 0., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [0., 0., 1., 0.],\n",
       "        [0., 0., 0., 1.]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testMat = mat(eye(4))\n",
    "testMat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0., 1., 0., 0.]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat0,mat1 = binSplitDataSet(testMat,1,0.5)\n",
    "mat0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[1., 0., 0., 0.],\n",
       "        [0., 0., 1., 0.],\n",
       "        [0., 0., 0., 1.]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.将CART算法用于回归\n",
    "* 如何计算连续型数值的混乱度:首先计算所有数据的均值，然后计算每条数据的值到均值的差值。为了对正负差值同等看待，一般使用绝对值或平方值来代替上述差值.(平方误差的总值（总方差）。总方差可以通过均方差乘以数据集中样本点的个数来得到)\n",
    "* chooseBestSplit():找到数据集切分的最佳位置。它遍历所有的特征及其可能的取值来找到使误差最小化的切分阈值。\n",
    "* leafType、errType和ops这三个参数:leafType是对创建叶节点的函数的引用，errType是对前面介绍的总方差计算函数的引用，而ops是一个用户定义的参数构成的元组，用以完成树的构建。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成叶节点。在回归树中，该模型其实就是目标变量的均值。 \n",
    "def regLeaf(dataSet): \n",
    "    return mean(dataSet[:,-1])\n",
    "\n",
    "# 误差估计函数：在给定数据上计算目标变量的平方误差\n",
    "def regErr(dataSet):\n",
    "    return var(dataSet[:,-1]) * shape(dataSet)[0] # 均方差乘以数据集中样本的个数\n",
    "\n",
    "# 回归树构建的核心函数\n",
    "def chooseBestSplit(dataSet, leafType=regLeaf, errType=regErr, ops=(1,4)):\n",
    "    tolS = ops[0]; tolN = ops[1]\n",
    "    # 如果所有值都相等则退出\n",
    "    if len(set(dataSet[:,-1].T.tolist()[0])) == 1: \n",
    "        return None, leafType(dataSet)             #exit cond 2\n",
    "    m,n = shape(dataSet)\n",
    "    # 误差S将用于与新切分误差进行对比，来检查新切分能否降低误差。\n",
    "    S = errType(dataSet)\n",
    "    bestS = inf; bestIndex = 0; bestValue = 0\n",
    "    for featIndex in range(n-1):\n",
    "        for splitVal in set(dataSet[:,featIndex].T.tolist()[0]):   # 与书中不同。即必须转换为行向量，然后再转换为列表，最后集合处理得到所有不同的元素值。\n",
    "            mat0, mat1 = binSplitDataSet(dataSet, featIndex, splitVal)\n",
    "            if (shape(mat0)[0] < tolN) or (shape(mat1)[0] < tolN): continue\n",
    "            newS = errType(mat0) + errType(mat1)\n",
    "            if newS < bestS: \n",
    "                bestIndex = featIndex\n",
    "                bestValue = splitVal\n",
    "                bestS = newS\n",
    "    # 如果误差减少不大则退出\n",
    "    if (S - bestS) < tolS: \n",
    "        return None, leafType(dataSet) #exit cond 2\n",
    "    mat0, mat1 = binSplitDataSet(dataSet, bestIndex, bestValue)\n",
    "    # 检查两个切分后的子集大小，如果某个子集的大小小于用户定义的参数tolN，则退出\n",
    "    if (shape(mat0)[0] < tolN) or (shape(mat1)[0] < tolN):  #exit cond 3\n",
    "        return None, leafType(dataSet)\n",
    "    return bestIndex,bestValue                  # 返回切分特征和特征值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'spInd': 0,\n",
       " 'spVal': 0.48813,\n",
       " 'left': 1.0180967672413792,\n",
       " 'right': -0.04465028571428572}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myDat = loadDataSet('ex00.txt')\n",
    "myMat = mat(myDat)\n",
    "createTree(myMat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'spInd': 1,\n",
       " 'spVal': 0.39435,\n",
       " 'left': {'spInd': 1,\n",
       "  'spVal': 0.582002,\n",
       "  'left': {'spInd': 1,\n",
       "   'spVal': 0.797583,\n",
       "   'left': 3.9871632,\n",
       "   'right': 2.9836209534883724},\n",
       "  'right': 1.980035071428571},\n",
       " 'right': {'spInd': 1,\n",
       "  'spVal': 0.197834,\n",
       "  'left': 1.0289583666666666,\n",
       "  'right': -0.023838155555555553}}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myDat1 = loadDataSet('ex0.txt')\n",
    "myMat1 = mat(myDat1)\n",
    "createTree(myMat1)  # 包含5个叶节点"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 树剪枝\n",
    "* 树的节点过多，则容易过拟合。\n",
    "* 前面章节“回归”中，使用了测试集上某种交叉验证技术来发现过拟合。\n",
    "* 通过降低决策树的复杂度来避免过拟合的过程称为剪枝（pruning）。\n",
    "* 在函数chooseBestSplit()中的提前终止条件，实际上是在进行一种所谓的预剪枝（prepruning）操作。另一种形式的剪枝需要使用测试集和训练集，称作后剪枝（postpruning）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 预剪枝\n",
    "* 预剪枝：chooseBestSplit()中的提前终止条件，即误差减少没那么多则提前终止。\n",
    "* 缺点：然而通过不断修改停止条件来得到合理结果并不是很好的办法。事实上，我们常常甚至不确定到底需要寻找什么样的结果。这正是机器学习所关注的内容，计算机应该可以给出总体的概貌。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 后剪枝\n",
    "* 利用测试集来对树进行剪枝。由于不需要用户指定参数，后剪枝是一个更理想化的剪枝方法。\n",
    "* 使用后剪枝方法需要将数据集分成测试集和训练集。首先指定参数，使得构建出的树足够大、足够复杂，便于剪枝。接下来从上而下找到叶节点，用测试集来判断将这些叶节点合并是否能降\n",
    "低测试误差。如果是的话就合并。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py37",
   "language": "python",
   "name": "py37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
