{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FP-growth算法\n",
    "* FP-growth算法能更高效地发现频繁项集，但不能用于发现关联规则。 \n",
    "* FP-growth算法只需要对数据库进行两次扫描，而Apriori算法对于每个潜在的频繁项集都会扫描数据集判定给定模式是否频繁，因此FP-growth算法的速度要比Apriori算法快。\n",
    "* 基本过程：1）构建FP树；2）从FP树中挖掘频繁项集。\n",
    "* 缺点：实现比较困难，在某些数据集上性能会下降"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* FP-growth算法将数据存储在一种称为FP树的紧凑数据结构中。FP代表频繁模式（Frequent Pattern）。一棵FP树看上去与计算机科学中的其他树结构类似，但是它通过链接（link）来连接相似元素，被连起来的元素项可以看成一个链表。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* FP-growth算法工作流程：对原始数据集扫描两遍。数据库的第一遍扫描用来统计出现的频率，而第二遍扫描中只考虑那些频繁元素。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.构建FP树\n",
    "* 在第二次扫描数据集时会构建一棵FP树。为构建一棵树，需要一个容器来保存树。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class treeNode:\n",
    "    def __init__(self, nameValue, numOccur, parentNode):\n",
    "        self.name = nameValue       # 存放节点名字\n",
    "        self.count = numOccur       # 节点出现次数\n",
    "        self.nodeLink = None       # 不同项集的相同项通过nodelink连接在一起\n",
    "        self.parent = parentNode   # 指向当前节点的父节点。\n",
    "        # parentNode通常情况下并不需要这个变量。因为通常是从上往下迭代访问节点的。后面的内容中需要根据给定叶子节点上溯整棵树，这时就需要指向父节点的指针。\n",
    "        self.children = {}         # 空字典变量，用于存放节点的子节点。\n",
    "    # 两个方法：inc()对count变量增加给定值，而另一个方法disp()用于将树以文本形式显示。后者对于树构建来说并不是必要的，但是它对于调试非常有用。 \n",
    "    def inc(self, numOccur):\n",
    "        self.count += numOccur\n",
    "        \n",
    "    def disp(self, ind=1):\n",
    "        print ('  '*ind, self.name, ' ', self.count)  # ind缩进\n",
    "        for child in self.children.values():\n",
    "            child.disp(ind+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "rootNode = treeNode('pyramid',9,None)\n",
    "rootNode.children['eye'] = treeNode('eye',13,None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   pyramid   9\n",
      "     eye   13\n"
     ]
    }
   ],
   "source": [
    "rootNode.disp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   pyramid   9\n",
      "     eye   13\n",
      "     phoenix   3\n"
     ]
    }
   ],
   "source": [
    "rootNode.children['phoenix'] = treeNode('phoenix',3,None)\n",
    "rootNode.disp()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.构建FP树\n",
    "* 头指针表：指向给定类型的第一个实例。使用字典的数据结构，还保存了每类元素的总数。利用头指针表，可以快速访问FP树中一个给定类型的所有元素。\n",
    "* 将集合添加到树之前，需要对每个集合进行排序。排序基于元素项的绝对出现频率来进行。\n",
    "* 在对事务记录过滤和排序之后，就可以构建FP树了。从空集开始，向其中不断添加频繁项集。过滤、排序后的事务依次添加到树中，如果树中已存在现有元素，则增加现有元素的值；如果现有元素不存在，则向树添加一个分枝。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createTree(dataSet, minSup=1): # 创建FP树不是挖掘\n",
    "    headerTable = {}\n",
    "    for trans in dataSet:         # 第一次扫描数据集\n",
    "        for item in trans:\n",
    "            headerTable[item] = headerTable.get(item, 0) + dataSet[trans] # get() 函数返回指定键的值（如指定键不存在则为0）\n",
    "    # 移除不满足最小支持度的元素项 \n",
    "    for k in list(headerTable):     # 原书中代码出现错误\n",
    "    # https://stackoverflow.com/questions/11941817/how-to-avoid-runtimeerror-dictionary-changed-size-during-iteration-error\n",
    "        if headerTable[k] < minSup: \n",
    "            del(headerTable[k])\n",
    "    freqItemSet = set(headerTable.keys())\n",
    "    # print 'freqItemSet: ',freqItemSet\n",
    "    if len(freqItemSet) == 0: return None, None  # 如果没有元素项满足要求，则退出\n",
    "    for k in headerTable:\n",
    "        headerTable[k] = [headerTable[k], None] \n",
    "    #print 'headerTable: ',headerTable\n",
    "    retTree = treeNode('Null Set', 1, None) # 创建树\n",
    "    for tranSet, count in dataSet.items():  # 第二次扫描数据集:根据全局频率（count）对每个事务中的元素进行排序\n",
    "        localD = {}\n",
    "        for item in tranSet:  \n",
    "            if item in freqItemSet:\n",
    "                localD[item] = headerTable[item][0]\n",
    "        if len(localD) > 0:\n",
    "            orderedItems = [v[0] for v in sorted(localD.items(), key=lambda p: p[1], reverse=True)]\n",
    "            updateTree(orderedItems, retTree, headerTable, count) # 使用排序后的频率项集对树进行填充\n",
    "    return retTree, headerTable "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def updateTree(items, inTree, headerTable, count):\n",
    "    if items[0] in inTree.children: # orderedItems[0] 是否在 retTree.children中\n",
    "        inTree.children[items[0]].inc(count) #incrament count\n",
    "    else:   # 增加 items[0] 到 inTree.children\n",
    "        inTree.children[items[0]] = treeNode(items[0], count, inTree)\n",
    "        if headerTable[items[0]][1] == None: # 更新header table \n",
    "            headerTable[items[0]][1] = inTree.children[items[0]]\n",
    "        else:\n",
    "            updateHeader(headerTable[items[0]][1], inTree.children[items[0]])\n",
    "    if len(items) > 1:   # 对剩下的元素项迭代调用update Tree函数\n",
    "        updateTree(items[1::], inTree.children[items[0]], headerTable, count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 该函数确保节点链接指向树中该元素项的每一个实例\n",
    "# 从头指针表的nodeLink开始，一直沿着nodeLink直到到达链表末尾。这就是一个链表。\n",
    "# 当处理树的时候，一种很自然的反应就是迭代完成每一件事。\n",
    "# 当以相同方式处理链表时可能会遇到一些问题，原因是如果链表很长可能会遇到迭代调用的次数限制。 \n",
    "def updateHeader(nodeToTest, targetNode):   # 不要使用递归遍历链表\n",
    "    while (nodeToTest.nodeLink != None):    \n",
    "        nodeToTest = nodeToTest.nodeLink\n",
    "    nodeToTest.nodeLink = targetNode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 返回一个事务列表\n",
    "def loadSimpDat():\n",
    "    simpDat = [['r', 'z', 'h', 'j', 'p'],\n",
    "               ['z', 'y', 'x', 'w', 'v', 'u', 't', 's'],\n",
    "               ['z'],\n",
    "               ['r', 'x', 'n', 'o', 's'],\n",
    "               ['y', 'r', 'x', 'z', 'q', 't', 'p'],\n",
    "               ['y', 'z', 'x', 'e', 'q', 's', 't', 'm']]\n",
    "    return simpDat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 一个项集中存在两个相同的元素不重复计数，因为frozenset()不含重复元素"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建树时会使用createTree()函数，而该函数的输入数据类型不是列表。\n",
    "# 因此需要createInitSet()用于实现上述从列表到字典的类型转换过程。（项集为字典中的键，而频率为每个键对应的取值。）\n",
    "def createInitSet(dataSet):\n",
    "    retDict = {}\n",
    "    for trans in dataSet:\n",
    "        retDict[frozenset(trans)] = 1\n",
    "    return retDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['r', 'z', 'h', 'j', 'p'],\n",
       " ['z', 'y', 'x', 'w', 'v', 'u', 't', 's'],\n",
       " ['z'],\n",
       " ['r', 'x', 'n', 'o', 's'],\n",
       " ['y', 'r', 'x', 'z', 'q', 't', 'p'],\n",
       " ['y', 'z', 'x', 'e', 'q', 's', 't', 'm']]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simpDat = loadSimpDat()\n",
    "simpDat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{frozenset({'h', 'j', 'p', 'r', 'z'}): 1,\n",
       " frozenset({'s', 't', 'u', 'v', 'w', 'x', 'y', 'z'}): 1,\n",
       " frozenset({'z'}): 1,\n",
       " frozenset({'n', 'o', 'r', 's', 'x'}): 1,\n",
       " frozenset({'p', 'q', 'r', 't', 'x', 'y', 'z'}): 1,\n",
       " frozenset({'e', 'm', 'q', 's', 't', 'x', 'y', 'z'}): 1}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initSet = createInitSet(simpDat)\n",
    "initSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建FP树\n",
    "myFPtree,myHeaderTab = createTree(initSet,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Null Set   1\n",
      "     z   5\n",
      "       r   1\n",
      "       x   3\n",
      "         t   3\n",
      "           s   2\n",
      "             y   2\n",
      "           r   1\n",
      "             y   1\n",
      "     x   1\n",
      "       s   1\n",
      "         r   1\n"
     ]
    }
   ],
   "source": [
    "# 使用disp()方法给出树的文本表示结果\n",
    "myFPtree.disp()     # 与书中结果其实是一致的：将y划分开"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'z': [5, <__main__.treeNode at 0x2c66d676508>],\n",
       " 'r': [3, <__main__.treeNode at 0x2c66d676c48>],\n",
       " 't': [3, <__main__.treeNode at 0x2c66d676908>],\n",
       " 's': [3, <__main__.treeNode at 0x2c66d676988>],\n",
       " 'y': [3, <__main__.treeNode at 0x2c66d676a48>],\n",
       " 'x': [4, <__main__.treeNode at 0x2c66d676d48>]}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myHeaderTab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.从一棵FP树中挖掘频繁项集\n",
    "* 有了FP树就可以抽取频繁项集了。思路与Apriori算法大致类似，首先从单元素项集合开始，然后在此基础上逐步构建更大的集合。\n",
    "* 利用FP树来做实现上述过程，不再需要原始数据集。 \n",
    "* 从FP树中抽取频繁项集的三个基本步骤如下：\n",
    "    * 从FP树中获得条件模式基；\n",
    "    * 利用条件模式基，构建一个条件FP树；\n",
    "    * 迭代重复步骤(1)步骤(2)，直到树包含一个元素项为止。\n",
    "* 接下来重点关注第(1)步，即寻找条件模式基的过程。之后，为每一个条件模式基创建对应的条件FP树。最后需要构造少许代码来封装上述两个函数，并从FP树中获得频繁项集。 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 抽取条件模式基\n",
    "* 首先从上一节发现的已经保存在头指针表中的单个频繁元素项开始。对于每一个元素项，获得其对应的条件模式基。\n",
    "* 条件模式基是以所查找元素项为结尾的路径集合。\n",
    "* 每一条路径其实都是一条前缀路径。简而言之，一条前缀路径是介于所查找元素项与树根节点之间的所有内容。 \n",
    "* 每一条前缀路径都与一个计数值关联。该计数值等于起始元素项的计数值（所查找元素项）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*　前缀路径将被用于构建条件FP树。现在先不考虑那些\n",
    "* 为了获得这些前缀路径，可以对树进行穷举式搜索，直到获得想要的频繁项为止，或者使用一个更有效的方法来加速搜索过程。\n",
    "* 可以利用先前创建的头指针表来得到一种更有效的方法。头指针表包含相同类型元素链表的起始指针。一旦到达了每一个元素项，就可以上溯这棵树直到根节点为止。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 发现以给定元素项结尾的所有路径的函数\n",
    "def ascendTree(leafNode, prefixPath): \n",
    "    if leafNode.parent != None:         # 迭代上溯整棵树\n",
    "        prefixPath.append(leafNode.name)\n",
    "        ascendTree(leafNode.parent, prefixPath)\n",
    "\n",
    "# 遍历链表直到到达结尾。每遇到一个元素项都会调用ascendTree()来上溯FP树，并收集所有遇到的元素项的名称\n",
    "def findPrefixPath(basePat, treeNode):   # treeNode来自于headertable\n",
    "    condPats = {}\n",
    "    while treeNode != None:\n",
    "        prefixPath = []\n",
    "        ascendTree(treeNode, prefixPath)\n",
    "        #print(prefixPath)\n",
    "        if len(prefixPath) > 1: \n",
    "            condPats[frozenset(prefixPath[1:])] = treeNode.count  # 前缀的count是：当前点的count\n",
    "        treeNode = treeNode.nodeLink                              # 链表连接的下一个\n",
    "    return condPats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['x', 'z']\n",
      "['x']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{frozenset({'z'}): 3}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "findPrefixPath('x',myHeaderTab['x'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['z']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "findPrefixPath('z',myHeaderTab['z'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['r', 'z']\n",
      "['r', 's', 'x']\n",
      "['r', 't', 'x', 'z']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{frozenset({'z'}): 1, frozenset({'s', 'x'}): 1, frozenset({'t', 'x', 'z'}): 1}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "findPrefixPath('r',myHeaderTab['r'][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2创建条件FP树"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 递归查找频繁项集的mineTree函数\n",
    "def mineTree(inTree, headerTable, minSup, preFix, freqItemList):\n",
    "    bigL = [v[0] for v in sorted(headerTable.items(), key=lambda p: p[1][0])]#源代码有误\n",
    "    for basePat in bigL:             # 从头指针表的底端开始\n",
    "        newFreqSet = preFix.copy()\n",
    "        newFreqSet.add(basePat)\n",
    "        #print 'finalFrequent Item: ',newFreqSet    \n",
    "        freqItemList.append(newFreqSet)\n",
    "        condPattBases = findPrefixPath(basePat, headerTable[basePat][1])\n",
    "        #print 'condPattBases :',basePat, condPattBases\n",
    "        # 从条件模式基来构建条件FP树\n",
    "        myCondTree, myHead = createTree(condPattBases, minSup)\n",
    "        #print 'head from conditional tree: ', myHead\n",
    "        if myHead != None: # 挖掘条件FP树\n",
    "            print ('conditional tree for: ',newFreqSet)\n",
    "            myCondTree.disp(1)            \n",
    "            mineTree(myCondTree, myHead, minSup, newFreqSet, freqItemList)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conditional tree for:  {'t'}\n",
      "   Null Set   1\n",
      "     z   3\n",
      "       x   3\n",
      "conditional tree for:  {'x', 't'}\n",
      "   Null Set   1\n",
      "     z   3\n",
      "conditional tree for:  {'s'}\n",
      "   Null Set   1\n",
      "     x   3\n",
      "conditional tree for:  {'y'}\n",
      "   Null Set   1\n",
      "     x   2\n",
      "       z   2\n",
      "         t   2\n",
      "     z   1\n",
      "       x   1\n",
      "         t   1\n",
      "conditional tree for:  {'x'}\n",
      "   Null Set   1\n",
      "     z   3\n"
     ]
    }
   ],
   "source": [
    "# 先建立一个空列表来存储所有的频繁项集：\n",
    "freqItems = []\n",
    "mineTree(myFPtree,myHeaderTab,3,set([]),freqItems)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 条件树：谁的条件树，就让谁与它的条件树的每个元素合并得到频繁项集。\n",
    "https://www.cnblogs.com/pinard/p/6307064.html\n",
    "* 我们很容易得到F的频繁2项集为{A:2,F:2}, {C:2,F:2}, {E:2,F:2}, {B:2,F:2}。**（从一个条件树中得到）**。\n",
    "递归合并二项集，得到频繁三项集为{A:2,C:2,F:2}，{A:2,E:2,F:2},...**（另外一个条件树）**。\n",
    "还有一些频繁三项集，就不写了。当然一直递归下去，最大的频繁项集为频繁5项集，为{A:2,C:2,E:2,B:2,F:2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'r'},\n",
       " {'t'},\n",
       " {'t', 'z'},\n",
       " {'t', 'x'},\n",
       " {'t', 'x', 'z'},\n",
       " {'s'},\n",
       " {'s', 'x'},\n",
       " {'y'},\n",
       " {'x', 'y'},\n",
       " {'y', 'z'},\n",
       " {'t', 'y'},\n",
       " {'x'},\n",
       " {'x', 'z'},\n",
       " {'z'}]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freqItems  # 返回项集与条件FP树相匹配。这个freqItems即符合支持度的频繁项集"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 例1：在Twitter源中发现一些共现词\n",
    "* twitter适合python36?\n",
    "* Twitter.com实际上是一个和其他人进行交流的通道，其上发表的内容被限制在140个字符以内，发表的一条信息称为推文。\n",
    "* 对于给定的搜索词，使用FP-growth算法来发现推文中的频繁单词集合。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def textParse(bigString):\n",
    "    urlsRemoved = re.sub('(http:[/][/]|www.)([a-z]|[A-Z]|[0-9]|[/.]|[~])*', '', bigString)    \n",
    "    listOfTokens = re.split(r'\\W*', urlsRemoved)\n",
    "    return [tok.lower() for tok in listOfTokens if len(tok) > 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = 'Open The Network, Says: In addition, RIMM needs to ... https://bit.ly/lvlVlU'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "urlsRemoved = re.sub('(http[s]:[/][/]|www.)([a-z]|[A-Z]|[0-9]|[/.]|[~])*', '', s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Open The Network, Says: In addition, RIMM needs to ... '"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urlsRemoved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Open',\n",
       " 'The',\n",
       " 'Network',\n",
       " '',\n",
       " 'Says',\n",
       " '',\n",
       " 'In',\n",
       " 'addition',\n",
       " '',\n",
       " 'RIMM',\n",
       " 'needs',\n",
       " 'to',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "listOfTokens = re.split(r'\\W', urlsRemoved)\n",
    "listOfTokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['open', 'the', 'network', 'says', 'addition', 'rimm', 'needs']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[tok.lower() for tok in listOfTokens if len(tok) > 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* FP树的构建是通过每次应用一个实例的方式来完成的。这里假设已经获得了所有数据，所以刚才是直接遍历所有数据来构建FP树的。实际上可以重写createTree()函数，每次读入一个实例，并随着Twitter流的不断输入而不断增长树。\n",
    "* FP-growth算法还有一个map-reduce版本的实现，它也很不错，可以扩展到多台机器上运行。Google使用该算法通过遍历大量文本来发现频繁共现词，其做法和我们刚才介绍的例子非常类似。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 例2：从新闻网站点击流中挖掘\n",
    "* 在更大的数据集上查看效果：在源数据集合中，有一个kosarak.dat文件，它包含将近100万条记录。\n",
    "* 该文件中的每一行包含某个用户浏览过的新闻报道。一些用户只看过一篇报道，而有些用户看过2498篇报道。\n",
    "* 用户和报道被编码成整数，所以查看频繁项集很难得到更多的东西，但是该数据对于展示FP-growth算法的速度十分有效。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "990002"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parsedDat = [line.split() for line in open('kosarak.dat').readlines()]\n",
    "len(parsedDat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对初始集合格式化： \n",
    "initSet = createInitSet(parsedDat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建FP树，并从中寻找那些至少被10万人浏览过的新闻报道:\n",
    "myFPtree, myHeaderTab = createTree(initSet,100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conditional tree for:  {'1'}\n",
      "   Null Set   1\n",
      "     6   107404\n",
      "conditional tree for:  {'3'}\n",
      "   Null Set   1\n",
      "     6   186289\n",
      "       11   117401\n",
      "     11   9718\n",
      "conditional tree for:  {'3', '11'}\n",
      "   Null Set   1\n",
      "     6   117401\n",
      "conditional tree for:  {'11'}\n",
      "   Null Set   1\n",
      "     6   261773\n"
     ]
    }
   ],
   "source": [
    "# 上述代码构建树以及扫描100万行只需要几秒钟，这展示了该算法的强大。\n",
    "# 创建一个空列表来保存这些频繁项集：\n",
    "myFreqList = []\n",
    "mineTree(myFPtree,myHeaderTab,100000,set([]),myFreqList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 有多少新闻报道或报道集合曾经被10万或者更多的人浏览过：共9个\n",
    "len(myFreqList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'1'},\n",
       " {'1', '6'},\n",
       " {'3'},\n",
       " {'11', '3'},\n",
       " {'11', '3', '6'},\n",
       " {'3', '6'},\n",
       " {'11'},\n",
       " {'11', '6'},\n",
       " {'6'}]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myFreqList  # 与书中结果一致"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 总结：\n",
    "* FP-growth算法是一种用于发现数据集中频繁模式的有效方法。FP-growth算法利用Apriori原则，执行更快。\n",
    "* 在FP-growth算法中，数据集存储在一个称为FP树的结构中。FP树构建完成后，可以通过查找元素项的条件基及构建条件FP树来发现频繁项集。该过程不断以更多元素作为条件重复进行，直到FP树只包含一个元素为止。\n",
    "* 可以使用FP-growth算法在多种文本文档中查找频繁单词。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py37",
   "language": "python",
   "name": "py37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
